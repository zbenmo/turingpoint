{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"turingpoint","text":"<p>Turing point is a Reinforcement Learning (RL) library. It adds the missing duct tape. It allows for multiple (hetrogenous) agents seamlessly. Per-agent partial observation is natural with Turing point. Different agents can act in differnet frequencies. You may opt to continue using also the environment and the agent libraries that you're currently using, for the such as Gym/Gymnasium, Stable-Baselines3, Tianshou, RLLib, etc. Turing point integrates easily with existing RL libraries and your own custom code. Integration of RL agents in the target applications should be significantly easier with Turing point.</p> <p>The main concept in Turing point is that there are multiple participants and each gets its turn. The participants communicate by a parcel that is passed among them. The agent and the environment are both participants in that sense. No more confusion which of those should call which. Reward's logic, for example, can be addressed where you believe is most suitable.</p> <p>Turing point may be helpful with parallel or distributed training, yet Turing point does not address those explicitly. On the contrary; with Turing point the flow is sequential among the participants. As far as we can tell Turing point at least does not hinder the use of parallel and/or distributed training.</p> <p>Participants can be added and/or removed dynamically (ex. a new \"monster\" enters or then \"disappears\").</p>"},{"location":"#participant","title":"Participant","text":"<p>Every component in the main \"episodic\" loop needs to implement the <code>Participant</code> protocol. This basically means that it should be a callable that receives a \"parcel\". The parcel is a dict, where one can expect to find things like action, reward, observation, etc.  </p> <p>In the example below, we define a \"participant\" that increments its own \"counter\" everytime it is being called.</p> <pre><code>def participant1(parcel: dict) -&gt; None:\nparcel['participant1'] = parcel.get('participant1', 0) + 1\n</code></pre> <p>Note the the parcel dict above could have potentailly have the obs key. It is just that participant1 did not check for it. On the other hand, if the key obs was there before the call, it should still be there in the parcel.</p> <p>The parcel is passed among the participants, each has an option to examine the contents and to add/modify/remove entries.</p> <p>The following participant (implemented in utils.py) can be used to collect rollouts:</p> <pre><code>class Collector:\n\"\"\"Simple \"participant\" that records specific values from the parcel.\n  \"\"\"\ndef __init__(self, keys_to_collect = ['obs', 'action', 'new_obs', 'reward', 'done']):\nself._keys_to_collect = keys_to_collect\nself._entries = [] # TODO: potentially replace with dqueue\ndef __call__(self, parcel: dict) -&gt; None:\nnew_entry = {k: parcel[k] for k in self._keys_to_collect}\nself._entries.append(new_entry)\ndef get_entries(self) -&gt; Generator[dict, None, None]:\nyield from self._entries\ndef clear_entries(self) -&gt; None:\nself._entries.clear()\n</code></pre>"},{"location":"#assembly","title":"Assembly","text":"<p>Assembly in turingpoint is an abstract class that enables an \"episodic\" loop. One needs to provide implementation for the following member functions: create_initial_parcel, and participants.</p> <p>get_participants is a callable that returns an iterable or an iterator for the participants. You can imagine a simple setting where you have a Gymnasium environment and a Stable-Baselines3 agent. The iterator shall return once the agent, and then the environment, and so force. In the example, both the environment and the agent needs to be wrapped as a participant. That is to adapt the step and predict API accordingly.</p> <pre><code>import functools\nimport itertools\nimport turingpoint.gymnasium_utils as tp_gym_utils\nimport turingpoint.sb3_utils as tp_sb3_utils\nimport turingpoint.utils as tp_utils\nimport turingpoint as tp\ndef evaluate(env, agent, num_episodes: int) -&gt; float:\ntotal_reward = 0\ndef bookkeeping(parcel: dict) -&gt; None:\nnonlocal total_reward\nreward = parcel['reward']\ntotal_reward += reward\ndef get_participants():\nyield functools.partial(tp_gym_utils.call_reset, env=env)\nyield from itertools.cycle([\nfunctools.partial(\ntp_sb3_utils.call_predict,\nagent=agent, deterministic=True\n),\nfunctools.partial(tp_gym_utils.call_step, env=env),\nbookkeeping,\ntp_gym_utils.check_done\n]) \nassembly = tp.Assembly(get_participants)\nfor _ in range(num_episodes):\n_ = assembly.launch()\nreturn total_reward / num_episodes\n</code></pre> <p>Note that launch is being called as many time as needed (as many episodes as needed). A new parcel is created (behind the scenes) for every call of launch. Also note that launch is returning the parcel as is at the end of the episode, yet we've decided above to discard it (discard the parcel and not look into it).</p>"},{"location":"self_play/","title":"Self-Play","text":"<p>Turing point includes also a template class for self-play, SelfPlay. SelfPlay in Turing point has its stand-alone interface and is not related to the participants/assembly main theme of the package.</p> <p>You can of course use both the SelfPlay template and participants/assembly launches as works for you in your application.</p> <p>For a SelfPlay you need to implement a few methods (abstract in the template class SelfPlay), and also to provide a couple of callables in the call to 'launch'.</p> <p>Abstract methods to implement:</p> <ul> <li>fetch_agent_to_train</li> <li>save_agent</li> <li>fetch_opponent</li> <li>train_against_agent</li> <li>evaluate_agent</li> </ul> <p>Callables to provide as arguments to 'launch':</p> <ul> <li>should_stop</li> <li>should_save</li> </ul> <p>Below is the code of 'launch':</p> <pre><code>def launch(self,\nshould_stop: Callable[[], bool],\nshould_save: Callable[[], bool]) -&gt; Quality:\nagent_to_train = self.fetch_agent_to_train()\nwhile not should_stop():\nopponent_agent = self.fetch_opponent()\nself.train_against_agent(agent_to_train, opponent_agent)\nif should_save():\nself.save_agent(agent_to_train)\nreturn self.evaluate_agent(agent_to_train)\n</code></pre>"}]}