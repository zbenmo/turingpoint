{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"turingpoint","text":"<p>Turing point is a Reinforcement Learning (RL) library. It adds the missing duct tape. It allows for multiple (hetrogenous) agents seamlessly. Per-agent partial observation is natural with Turing point. Different agents can act in differnet frequencies. You may opt to continue using also the environment and the agent libraries that you're currently using, for the such as Gym/Gymnasium, Stable-Baselines3, Tianshou, RLLib, etc. Turing point integrates easily with existing RL libraries and your own custom code. Integration of RL agents in the target applications should be significantly easier with Turing point.</p> <p>The main concept in Turing point is that there are multiple participants and each gets its turn. The participants communicate by a parcel that is passed among them. The agent and the environment are both participants in that sense. No more confusion which of those should call which. Reward's logic, for example, can be addressed where you believe is most suitable.</p> <p>Turing point may be helpful with parallel or distributed training, yet Turing point does not address those explicitly. On the contrary; with Turing point the flow is sequential among the participants. As far as we can tell Turing point at least does not hinder the use of parallel and/or distributed training.</p> <p>Participants can be added and/or removed dynamically (ex. a new \"monster\" enters or then \"disappears\").</p>"},{"location":"#participant","title":"Participant","text":"<p>Every component in the main \"episodic\" loop needs to implement the <code>Participant</code> protocol. This basically means that it should be a callable that receives a \"parcel\". The parcel is a dict, where one can expect to find things like action, reward, observation, etc.  </p> <p>In the example below, we define a \"participant\" that increments its own \"counter\" everytime it is being called.</p> <pre><code>def participant1(parcel: dict) -&gt; None:\nparcel['participant1'] = parcel.get('participant1', 0) + 1\n</code></pre> <p>Note the the parcel dict above could have potentailly have the obs key. It is just that participant1 did not check for it. On the other hand, if the key obs was there before the call, it should still be there in the parcel.</p> <p>The parcel is passed among the participants, each has an option to examine the contents and to add/modify/remove entries.</p> <p>The following participant (implemented in utils.py) can be used to collect rollouts:</p> <pre><code>class Collector:\n\"\"\"Simple \"participant\" that records specific values from the parcel.\n  \"\"\"\ndef __init__(self, keys_to_collect = ['obs', 'action', 'new_obs', 'reward', 'done']):\nself._keys_to_collect = keys_to_collect\nself._entries = [] # TODO: potentially replace with dqueue\ndef __call__(self, parcel: dict) -&gt; None:\nnew_entry = {k: parcel[k] for k in self._keys_to_collect}\nself._entries.append(new_entry)\ndef get_entries(self) -&gt; Generator[dict, None, None]:\nyield from self._entries\ndef clear_entries(self) -&gt; None:\nself._entries.clear()\n</code></pre>"},{"location":"#assembly","title":"Assembly","text":"<p>Assembly in turingpoint is an abstract class that enables an \"episodic\" loop. One needs to provide implementation for the following member functions: create_initial_parcel, and participants.</p> <p>participants is participant generator. You can imagine a simple setting where you have a Gymnasium environment and a Stable-Baselines3 agent. The generator shall return once the agent, and then the environment, and so force. In the example, both the environment and the agent needs to be wrapped as a participant. That is to adapt the step and predict API accordingly.</p> <p>create_initial_parcel needs to return a dict with the initial observation for example (from env.reset()).</p> <pre><code>def evaluate(num_episodes: int) -&gt; float:\ntotal_reward = 0\ndef bookkeeping(parcel: dict) -&gt; None:\nnonlocal total_reward\nreward = parcel['reward']\ntotal_reward += reward\nassembly = GymnasiumAssembly(env, [\nAgentParticipant(agent, deterministic=True),\nEnvironmentParticipant(env),\nbookkeeping\n])\nfor _ in range(num_episodes):\n_ = assembly.launch()\nreturn total_reward / num_episodes\n</code></pre> <p>In the example above GymnasiumAssembly is an implementation of the Assembly class that accepts a Gymnasium environment and a list of participants (usually first the agent, then the environment, and then potentially more). GymnasiumAssembly also has an implementation of create_initial_parcel, and it also adds an internal participant that checks for termination, and so participants generator of GymnasiumAssembly should return when relevant.</p> <p>Note that launch is being called as many time as needed (as many episodes as needed). A new parcel is created (behind the scenes) for every call of launch. Also note that launch is returning the parcel as is at the end of the episode, yet we've decided above to discard it (discard the parcel and not look into it).</p>"},{"location":"self_play/","title":"Self-Play","text":"<p>Turing point includes also a template class for self-play, SelfPlay. SelfPlay in Turing point has its stand-alone interface and is not related to the participants/assembly main theme of the package.</p> <p>You can of course use both the SelfPlay template and participants/assembly launches as works for you in your application.</p> <p>For a SelfPlay you need to implement a few methods (abstract in the template class SelfPlay), and also to provide a couple of callables in the call to 'launch'.</p> <p>Abstract methods to implement:</p> <ul> <li>fetch_agent_to_train</li> <li>save_agent</li> <li>fetch_opponent</li> <li>train_against_agent</li> <li>evaluate_agent</li> </ul> <p>Callables to provide as arguments to 'launch':</p> <ul> <li>should_stop</li> <li>should_save</li> </ul> <p>Below is the code of 'launch':</p> <pre><code>def launch(self,\nshould_stop: Callable[[], bool],\nshould_save: Callable[[], bool]) -&gt; Quality:\nagent_to_train = self.fetch_agent_to_train()\nwhile not should_stop():\nopponent_agent = self.fetch_opponent()\nself.train_against_agent(agent_to_train, opponent_agent)\nif should_save():\nself.save_agent(agent_to_train)\nreturn self.evaluate_agent(agent_to_train)\n</code></pre>"}]}